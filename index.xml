<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home on ICASSP 2023: Auditory EEG challenge</title>
    <link>https://exporl.github.io/auditory-eeg-challenge-2023/</link>
    <description>Recent content in Home on ICASSP 2023: Auditory EEG challenge</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://exporl.github.io/auditory-eeg-challenge-2023/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Timeline</title>
      <link>https://exporl.github.io/auditory-eeg-challenge-2023/homepage_sections/timeline/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://exporl.github.io/auditory-eeg-challenge-2023/homepage_sections/timeline/</guid>
      <description>Timeline (anywhere on earth) November 21, 2022: Registration opens November 21, 2022: Release of training set, code, baseline methods and documentation January 6, 2023: Release of evaluation test set February 6, 2023: Deadline for submitting results for both tasks February 20, 2023: ICASSP 2023 grand challenge 2-page paper deadline (top 5 teams only) March 7, 2023: ICASSP 2023 grand challenge 2-page paper acceptance notification March 14, 2023: ICASSP 2023 grand challenge 2-page camera-ready deadline June 4-8, 2023: ICASSP 2023 conference August 9, 2023: IEEE open journal of signal processing grand challenge papers deadline </description>
    </item>
    
    <item>
      <title>Dataset</title>
      <link>https://exporl.github.io/auditory-eeg-challenge-2023/dataset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://exporl.github.io/auditory-eeg-challenge-2023/dataset/</guid>
      <description>The training set can be downloaded here, using the password which will be provided to all registered teams: ICASSP-2023-eeg-decoding-challenge-dataset
EEG Electroencephalography (EEG) is a non-invasive method to record electrical activity in the brain, which is generated by ionic currents that flow within and across neuron cells. When a large population of thousands or millions of neurons with a similar orientation in a specific brain region synchronises its electrical activity, the produced electrical field is large enough to be observable on the scalp.</description>
    </item>
    
    <item>
      <title>General description</title>
      <link>https://exporl.github.io/auditory-eeg-challenge-2023/homepage_sections/general_description/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://exporl.github.io/auditory-eeg-challenge-2023/homepage_sections/general_description/</guid>
      <description>Challenge Call Various neuroimaging techniques can be used to investigate how the brain processes sound. Electroencephalography (EEG) is popular because it is relatively easy to conduct and has a high temporal resolution. Besides fundamental neuroscience research, EEG-based measures of auditory processing in the brain are also helpful in detecting or diagnosing potential hearing loss. They enable differential diagnosis of populations that can otherwise not be tested, such as young children or people with mental disabilities.</description>
    </item>
    
    <item>
      <title>leaderboard</title>
      <link>https://exporl.github.io/auditory-eeg-challenge-2023/task1/leaderboard/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://exporl.github.io/auditory-eeg-challenge-2023/task1/leaderboard/</guid>
      <description> All results Group nameSubmission number Within subjects mean Within subjects std Heldout subjects mean Heldout subjects std total score </description>
    </item>
    
    <item>
      <title>leaderboard</title>
      <link>https://exporl.github.io/auditory-eeg-challenge-2023/task2/leaderboard/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://exporl.github.io/auditory-eeg-challenge-2023/task2/leaderboard/</guid>
      <description> All results Group nameSubmission number Within subjects mean Within subjects std Heldout subjects mean Heldout subjects std total score </description>
    </item>
    
    <item>
      <title>Organizers</title>
      <link>https://exporl.github.io/auditory-eeg-challenge-2023/homepage_sections/organizers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://exporl.github.io/auditory-eeg-challenge-2023/homepage_sections/organizers/</guid>
      <description>Organizers Mohammad Jalilpour Monesi 1, 2 (mohammad.jalilpourmonesi@esat.kuleuven.be)
Lies Bollens 1, 2 (lies.bollens@esat.kuleuven.be)
Bernd Accou 1, 2
Hugo Van hamme 1
Tom Francart 2
KU Leuven, PSI, Dept. of Electrical engineering (ESAT), Leuven, Belgium
KU Leuven, ExpORL, Dept. Neurosciences, Leuven, Belgium</description>
    </item>
    
    <item>
      <title>Registration</title>
      <link>https://exporl.github.io/auditory-eeg-challenge-2023/registration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://exporl.github.io/auditory-eeg-challenge-2023/registration/</guid>
      <description>Registration Teams can register by sending a mail to auditoryeegchallenge@kuleuven.be with the names of the team members, emails, and affiliations. Upon confirmation, teams will receive access to the training data.
Guidelines for participants Participants can submit their predictions up to five times. The latest received submission counts as the official score. The Audio-EEG challenge features two separate tasks. Participants can submit to either one track or both. Results should be accom- panied by a 2-page paper describing the proposed method The top 5 ranked teams will be invited to submit a 2-page paper, to be presented at ICASSP-2023, which should be submitted before the camera-ready deadline.</description>
    </item>
    
    <item>
      <title>Task 1: Match-mismatch</title>
      <link>https://exporl.github.io/auditory-eeg-challenge-2023/task1/description/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://exporl.github.io/auditory-eeg-challenge-2023/task1/description/</guid>
      <description>Schematic overview of the match-mismatch task Description Task 1 is a classification problem in a match-mismatch paradigm. In this paradigm, three inputs are presented to the model:
a segment of EEG, the time-aligned speech stimulus (match) an imposter stimulus (mismatch) The task of the model is to determine which of the input stimulus segments corresponds to the EEG. The performance metric is the classification accuracy (%).
The input length of all (EEG, envelope) pairs is 3s.</description>
    </item>
    
    <item>
      <title>Task 2: Regression</title>
      <link>https://exporl.github.io/auditory-eeg-challenge-2023/task2/description/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://exporl.github.io/auditory-eeg-challenge-2023/task2/description/</guid>
      <description>Schematic overview of the regression task Description Task 2 is a regression problem: To reconstruct the stimulus from the EEG. After reconstruction, a metric to measure the similarity is used between the reconstructed stimulus and the original stimulus. In this task, we use the Pearson correlation.
For this task, the stimulus representation is defined as the envelope, as described in the preprocessing section and as defined by the provided code.</description>
    </item>
    
    <item>
      <title>Test Set Task 1</title>
      <link>https://exporl.github.io/auditory-eeg-challenge-2023/task1/test_set/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://exporl.github.io/auditory-eeg-challenge-2023/task1/test_set/</guid>
      <description>Provided test dataset The test data for this task can be downloaded from here. The password is the same one as provided for the training set. The directory contains following items:
Preprocessed EEG test samples: The main directory contains 84 subjects&amp;rsquo; test data in the format of subject_name.json. Note that there is no test data for sub-001. Each json file contains a python dictionary with sample IDs as keys and tuples in the format of (EEG, speech1, speech2) as values.</description>
    </item>
    
    <item>
      <title>Test Set Task 2</title>
      <link>https://exporl.github.io/auditory-eeg-challenge-2023/task2/test_set/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://exporl.github.io/auditory-eeg-challenge-2023/task2/test_set/</guid>
      <description>Provided test data The test data for this task can be downloaded from here. The password is the same one as provided for the training set. The directory contains the following items:
Preprocessed EEG test samples: The main directory contains 84 subjects&amp;rsquo; test data in the format of subject_name.json. Note that there is no test data for sub-001. Each json file contains a python dictionary with samples IDs as keys and EEG segments of one minute as values.</description>
    </item>
    
    <item>
      <title>Get started</title>
      <link>https://exporl.github.io/auditory-eeg-challenge-2023/homepage_sections/leaderboard/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://exporl.github.io/auditory-eeg-challenge-2023/homepage_sections/leaderboard/</guid>
      <description>Get started Send a mail to auditoryeegchallenge@kuleuven.be with the names of the team members, emails, and affiliations. You will receive a password to download the data
Download the data from ICASSP-2023-eeg-decoding-challenge-dataset
split_data(.zip) contains already preprocessed, split and normalized data; ready for model training/evaluation. If you want to get started quickly, you can opt to only download this folder/zipfile. preprocessed_eeg(.zip) and preprocessed_stimuli(.zip) contain preprocessed EEG and stimuli files ( speech envelope and mel spectrogram features) respectively.</description>
    </item>
    
  </channel>
</rss>
